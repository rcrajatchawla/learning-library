# Getting Started

## Integrate with Data Catalog MetaStore

This tutorial introduces you to build a sample python spark application to integrate Data Flow with Data Catalog Metastore . Once build, you will learn how to run the application on Oracle Cloud Infrastructure Data Flow, a fully managed service that lets you run any Apache Spark Application at any scale with no infrastructure to deploy or manage. This tutorial requires basic knowledge of python spark. All the data used in the tutorial have been provided for you.

*Estimated Lab Time*: 40 minutes

### Objectives

For integration with OCI Data Flow, the Metastore provides an invocation endpoint to OCI Data Flow, exposing the Hive Metastore interface. With this integration it is possible to

Apache Hive is a data warehousing framework that facilitates read, write, or manage operations on large datasets residing in distributed systems. A Hive Metastore is the central repository of metadata for a Hive cluster. It stores metadata for data structures such as databases, tables, and partitions in a relational database, backed by files maintained in Object Storage. Apache Spark SQL makes use of a Hive Metastore for this purpose.

In this lab, you will do the following :

1. Navigate to Data Catalog Console and creates new MetaStore Instance under a Compartment.

2. Create policies required to execute metaservice calls against the metastore instance created.

3. Create policies required to add/update/delete objects in the bucket that will be used as the data warehouse location.

4. Build a sample PySpark application that will create the database and DDL to create HIVE Managed Table. We run some select/ aggregation queries to see that the metadata about the data is stored in the metaStore and all subsequent data flow Runs, can leverage the DDL created once and query the data.

5. Navigates to Data Flow Console and creates the Data Flow Application using the PySpark application created in #4 above and selects the metastore that needs to be associated with the template.

6. User navigates to Data Flow Console and runs the Application.

Your dataset is the [Yelp Review and Business Dataset](https://www.kaggle.com/yelp-dataset/yelp-dataset), downloaded from the Kaggle website under the terms of the Creative Commons CC0 1.0 Universal (CC0 1.0) "Public Domain Dedication" license.

The dataset is in JSON format and it is stored  in object store for downstream processing.

This lab guides you step by step, and provides the parameters you need. The python application is uploaded to the [Bucket](https://objectstorage.us-ashburn-1.oraclecloud.com/n/bigdatadatasciencelarge/b/dataflow-code/o/data-cleansing%2Fdatacleaning.py)

  ![Lab Overview](../images/ " ")

### Prerequisites

* Python 3.6+ setup locally.

* An Oracle Cloud log in with the API Key capability enabled. Load your user under Identity/Users, and confirm you can create API Keys.

![API Keys](../images/api-keys-yes.png " ")

* An API key registered and deployed to your local environment. See [Register an API Key](https://docs.oracle.com/iaas/Content/API/Concepts/apisigningkey.htm) for more information

* OCI Python SDK. See [Installing OCI python SDK](https://oracle-cloud-infrastructure-python-sdk.readthedocs.io/en/latest/installation.html#downloading-and-installing-the-sdk)

* A python IDE of your choice. The workshop uses [Visual Studio Code (VSCode)](https://code.visualstudio.com/download)

* Local environment setup with all the dependencies setup as described [Here](https://docs.oracle.com/en-us/iaas/data-flow/data-flow-tutorial/develop-apps-locally/front.htm#develop-locally-concepts)

* From the Console, click the hamburger menu to display the list of available services. Select Data Flow and click `Applications`

* Basic understanding of Python and Spark.

## **STEP 2**: Build a Sample PySpark Application to create Managed and UnManaged Tables

1. Begin by importing the python modules

    ```python
       <copy>
       import os
       import traceback
       from pyspark import SparkConf
       from pyspark.sql import SparkSession
       from pyspark.sql.context import SQLContext
       from pyspark.sql.functions import *
       from pyspark.sql.types import *
     </copy>
    ```

2. In the main function we start with the program logic. The main function is a starting point of any python program. When the program is run, the python interpreter runs the code sequentially. As input we pass the the location of the object storage location that has the  netflix csv file.

     ```python
     <copy>
         if __name__ == "__main__":
         main()
     </copy>
    ```

    ```python
     <copy>

    def main():

   # Input Parquet files
    YELP_REVIEW_INPUT_PATH = sys.argv[1]
    YELP_BUSINESS_INPUT_PATH = sys.argv[2]
    db_name = sys.argv[3]

    YELP_REVIEW_OUTPUT_PATH = os.path.dirname(YELP_REVIEW_INPUT_PATH) + "/parquet"
    YELP_BUSINESS_OUTPUT_PATH = os.path.dirname(YELP_BUSINESS_INPUT_PATH) + "/parquet"


    # Set up Spark.
    # Set up Spark.
    spark_session = get_dataflow_spark_session()

    spark_session.sql('show databases').show()

    # Load our data.

    review_input_dataframe = spark_session.read.option("header", "true").option(
        "mergeSchema", "true").json(YELP_REVIEW_INPUT_PATH)

    review_input_dataframe = flatten(review_input_dataframe)

    review_input_dataframe.printSchema()

    business_input_dataframe = spark_session.read.option("header", "true").option(
        "mergeSchema", "true").json(YELP_BUSINESS_INPUT_PATH)

    business_input_dataframe = flatten(business_input_dataframe)

    business_input_dataframe.printSchema()

    # Create Hive External Table
    createHiveTable(spark_session, review_input_dataframe,
                    business_input_dataframe, YELP_REVIEW_OUTPUT_PATH, YELP_BUSINESS_OUTPUT_PATH, db_name)

    def flatten(df):
    # compute Complex Fields (Lists and Structs) in Schema
    complex_fields=dict([(field.name, field.dataType)
                           for field in df.schema.fields
                           if type(field.dataType) == ArrayType or type(field.dataType) == StructType])
    while len(complex_fields) != 0:
        col_name=list(complex_fields.keys())[0]
        print("Processing :"+col_name+" Type : " +
              str(type(complex_fields[col_name])))

        # if StructType then convert all sub element to columns.
        # i.e. flatten structs
        if (type(complex_fields[col_name]) == StructType):
            expanded=[col(col_name+'.'+k).alias(col_name+'_'+k)
                        for k in [n.name for n in complex_fields[col_name]]]
            df=df.select("*", *expanded).drop(col_name)

        # if ArrayType then add the Array Elements as Rows using the explode function
        # i.e. explode Arrays
        elif (type(complex_fields[col_name]) == ArrayType):
            df=df.withColumn(col_name, explode_outer(col_name))

        # recompute remaining Complex Fields in Schema
        complex_fields=dict([(field.name, field.dataType)
                               for field in df.schema.fields
                               if type(field.dataType) == ArrayType or type(field.dataType) == StructType])
    return df                
     </copy>
    ```

3. Next, create a spark session and in the session config, add the OCI configuration. We pass the following configuration

     1. OCID of the user calling the API. To get the value, see [Required Keys and OCIDs](https://docs.oracle.com/en-us/iaas/Content/API/Concepts/apisigningkey.htm#Required_Keys_and_OCIDs).  

     2. Fingerprint for the public key that was added to this user. To get the value, see[Required Keys and OCIDs](https://docs.oracle.com/en-us/iaas/Content/API/Concepts/apisigningkey.htm#Required_Keys_and_OCIDs)  

     3. Full path and filename of the private key. The key pair must be in the PEM format. For instructions on generating a key pair in PEM forat, see [Required Keys and OCIDs](https://docs.oracle.com/en-us/iaas/Content/API/Concepts/apisigningkey.htm#Required_Keys_and_OCIDs)

     4. Passphrase used for the key, if it is encrypted.

     5. OCID of your tenancy. To get the value, see [Required Keys and OCIDs](https://docs.oracle.com/en-us/iaas/Content/API/Concepts/apisigningkey.htm#Required_Keys_and_OCIDs)

     6. An Oracle Cloud Infrastructure region. see [Regions and Availability Domains](https://docs.oracle.com/en-us/iaas/Content/General/Concepts/regions.htm#top)

    ```python
    <copy>
    def get_dataflow_spark_session(
    app_name="DataFlow", file_location=None, profile_name=None, spark_config={}
    ):
    """
    Get a Spark session in a way that supports running locally or in Data Flow.
    """
    if in_dataflow():
        spark_builder = SparkSession.builder.appName(app_name)
    else:
        # Import OCI.
        try:
            import oci
        except:
            raise Exception(
                "You need to install the OCI python library to test locally"
            )
        # Use defaults for anything unset.
        if file_location is None:
            file_location = oci.config.DEFAULT_LOCATION
        if profile_name is None:
            profile_name = oci.config.DEFAULT_PROFILE

        # Load the config file.
        try:
            oci_config = oci.config.from_file(
                file_location=file_location, profile_name=profile_name
            )
        except Exception as e:
            print("You need to set up your OCI config properly to run locally")
            raise e
        conf = SparkConf()
        conf.set("fs.oci.client.auth.tenantId", oci_config["tenancy"])
        conf.set("fs.oci.client.auth.userId", oci_config["user"])
        conf.set("fs.oci.client.auth.fingerprint", oci_config["fingerprint"])
        conf.set("fs.oci.client.auth.pemfilepath", oci_config["key_file"])
        conf.set(
            "fs.oci.client.hostname",
            "https://objectstorage.{0}.oraclecloud.com".format(oci_config["region"]),
        )
        spark_builder = SparkSession.builder.appName(app_name).config(conf=conf)

    # Add in extra configuration.
    for key, val in spark_config.items():
        spark_builder.config(key, val)

    # Create the Spark session. Enables Hive support, including connectivity to a persistent Hive metastore
     session = spark_builder.enableHiveSupport().getOrCreate()
    return session
    </copy>
    ```

4. Create Hive Managed Table.

```python
    <copy>
   def createHiveTable(spark_session, review_input_dataframe, business_input_dataframe, YELP_REVIEW_OUTPUT_PATH, YELP_BUSINESS_OUTPUT_PATH, db_name):

    try:
        spark_session.sql("CREATE DATABASE IF NOT EXISTS " + db_name)
    except:
        print(traceback.format_exc())
        print(traceback.print_stack())

    spark_session.sql("USE " + db_name)

    try:

        ddl = build_hive_ddl(review_input_dataframe, "yelp_review",
                             YELP_REVIEW_OUTPUT_PATH)
        spark_session.sql(ddl)
        review_input_dataframe.write.mode("overwrite").saveAsTable(
            db_name + ".yelp_review")
    except Exception as err:
        print(traceback.format_exc())
        print(traceback.print_stack())

    try:

        ddl = build_hive_ddl(
            business_input_dataframe, "yelp_business_raw", YELP_BUSINESS_OUTPUT_PATH)
        spark_session.sql(ddl)
        business_input_dataframe.write.mode("overwrite").saveAsTable(
            db_name + ".yelp_business_raw")
    except Exception as err:
        print(traceback.format_exc())
        print(traceback.print_stack())
        print("Table yelp_business_raw already exists")

    spark_session.sql("SHOW Tables").show()
    spark_session.sql("DESCRIBE TABLE yelp_review").show()
    spark_session.sql("DESCRIBE TABLE yelp_business_raw").show()
    spark_session.sql("SELECT (*) FROM yelp_review").show()
    spark_session.sql("SELECT * from yelp_business_raw limit 5").show()
    spark_session.sql("SELECT * from yelp_review limit 5;").show()
    spark_session.sql(
        "CREATE TABLE IF NOT EXISTS Restaurant_by_review_500 AS SELECT name, state, city, review_count from yelp_business_raw WHERE review_count > 500 order by review_count")
    spark_session.sql("SELECT * from Restaurant_by_review_500 limit 5;").show()


def build_hive_ddl(df, table_name, location):
    object_schema = df.schema
    columns = (','.join([field.simpleString()
               for field in object_schema])).replace(':', ' ')
    ddl = 'CREATE EXTERNAL TABLE '+table_name+' ('\
        + columns + ')'\
        + ' STORED AS PARQUET'\
        + ' LOCATION "'+location+'"'

    print('Generated Hive DDL:\n'+ddl)
    return ddl
    </copy>
 ```

## **STEP 3**: Inspect the Input JSON DataSet

 1. From the Console navigate to the object storage
   ![Object Store](../images/upload_objecstorage.png " ")

 2. For this workshop, we are reusing existing buckets `workshop-scripts` that contains all the input python files and `workshop-data` that contains the Input JSON File  in compartment  `dataflow-demo`. The process to upload the files is as described [File Upload](https://docs.oracle.com/en-us/iaas/Content/GSG/Tasks/addingbuckets.htm#Putting_Data_into_Object_Storage).

 3. In the `workshop-scripts` find the `hiveexample.py` file

     ![Files](../images/Hive-Example.png " ")

 4. In the `workshop-data` find the input JSON files ```yelp_business_yelp_academic_dataset_business.json``` in the folder `yelp_business` and the file ```yelp_review_yelp_academic_dataset_review.json``` in the folder ```yelp_review```

     ![Yelp JSON Files](../images/Yelp-Input-JSON-Dataset.png " ")

## **STEP 4**: Inspect the MetaStore

1. In the Console, open the navigation menu and click ```Analytics & AI```. Click ```Data Catalog```.

    ![Data Catalog](../images/data-catalog.png " ")

2. From the Data Catalog service page, click ```Metastores```.

    ![MetaStore](../images/MetaStore.png " ")

3. Change the Compartment to the ```dataflow-demo```

   ![MetaStore Compartment](../images/Metastore-Change-Compartment.png " ")

4. Should list the MetaStores in  ```dataflow-demo``` Compartment and you should find the ```DF-metastore``` metastore that we will use for the lab

    ![Show MetaStore](../images/metastore-compartment.png " ")

5. Open the metastore ```DF-metastore```  to inspect the location of the Managed table for the MetaStore. This will hold the data for the Managed table that we create later.

    ![DF Metastore](../images/DF-metastore.png " ")

## **STEP 4**: Create a Python application in Oracle Cloud Infrastructure Data Flow

1. Create an Application, provide a `Name` and `Description`

2. In the `Resource Configuration`. Leave the default values for `Spark Version`, `Driver Shape` , `Exexutor Shape` and `Number of Executors`

3. In the Application Configuration

    3.1 Choose `Language` as `Python`

    3.2 In `Select a File` pick the Object Storage File Name bucket as `Field-Training` and file name as `hiveexample.py`

    3.3 Leave the `Main class Name` and the `Archive URI` empty

    3.4 In the `Argument` field enter the path for the JSON files (seperated by space) that we saw in Step#3 and name of Hive DB (give your UUID + "DB"). The format of path is as following ```oci://<<bucketname@namespace/folder-path>>```

    for e.g. `oci://workshop-data@idehhejtnbtc/yelp_review/yelp_review_yelp_academic_dataset_review.json oci://workshop-data@idehhejtnbtc/yelp_business/yelp_business_yelp_academic_dataset_business.json FieldTrainingDB`

    3.5 In the `Metastore in dataflow-demo` choose ```DF-metastore`` and you should the path of the Managed table getting populated automatically.

4. Double-check your Application configuration, and confirm it is similar to the following

   ![Sample Application Configuration](../images/Hive-MetaStore-Application.png " ")

   ![Sample Application Configuration](../images/Hive-MetaStore-Application-2.png " ")

5. When done, click **Create**. When the Application is created, you see it in the Application list.

   ![Sample Application](../images/MetaStore-App.png " ")

## **STEP 5**: Run the Oracle Cloud Infrastructure Data Flow application

1. Highlight your Application in the list, click the **Actions** icon, and click **Run**

   ![Run Sample Application](../images/Run-Hive-MetaStore.png " ")

2. While the Application is running, you can optionally load the **Spark UI**  to monitor progress. From the **Actions** icon for the run in question, select **Spark UI**

   ![Run Spark UI](../images/hive-metastore-run.png " ")

3. You are automatically redirected to the Apache Spark UI, which is useful for debugging and performance tuning.

   ![Spark UI](../images/SparkUI-Hive-MetaStore..png " ")

4. After few mins your `Data Flow Run`  should show successful completion with a State of Succeeded:

   ![Run Succeeded](../images/hive-metastore-run-success.png " ")

5. Drill into the Run to see more details, and scroll to the bottom to see a listing of logs.

   ![Run logs](../images/hive-metastore-log-run.png " ")

6. When you click the `spark_application_stdout.log.gz`  file, you should see the following log output

   ![Application logs](../images/Hive-MetaStore-log1.png " ")

   ![Application logs](../images/Hive-MetaStore-log2.png " ")

   ![Application logs](../images/Hive-MetaStore-log3.png " ")

7. Navigate to the Object Store that was used as location for the tables and look at its content. It shows the table and the parquet files

   ![Application logs](../images/Table-Location-Object-Store.png " ")

## **STEP 6**: Connect to the Managed Table from Seperate Application

1. Create a Python Application to connect to the Managed table. Application connects to the database that was created in previous step, creates a dataframe and perform operations on the dataframe.

```python
<copy>
import os
import sys 
import traceback

from pyspark import SparkConf
from pyspark.sql import SparkSession
from pyspark.sql.context import SQLContext
from pyspark.sql.functions import *
from pyspark.sql.types import *


def main():

    # Set up Spark.
    spark_session = get_dataflow_spark_session()
    db_name = sys.argv[1]
    queryMetaStore(spark_session, db_name)
    
def get_dataflow_spark_session(app_name="DataFlow", file_location=None, profile_name=None, spark_config={}):
    """
    Get a Spark session in a way that supports running locally or in Data Flow.
    """
    if in_dataflow():
        spark_builder=SparkSession.builder.appName(app_name)
    else:
        # Import OCI.
        try:
            import oci
        except:
            raise Exception(
                "You need to install the OCI python library to test locally"
            )

        # Use defaults for anything unset.
        if file_location is None:
            file_location=oci.config.DEFAULT_LOCATION
        if profile_name is None:
            profile_name=oci.config.DEFAULT_PROFILE

        # Load the config file.
        try:
            oci_config=oci.config.from_file(
                file_location=file_location, profile_name=profile_name
            )
        except Exception as e:
            print("You need to set up your OCI config properly to run locally")
            raise e
        conf=SparkConf()
        conf.set("fs.oci.client.auth.tenantId", oci_config["tenancy"])
        conf.set("fs.oci.client.auth.userId", oci_config["user"])
        conf.set("fs.oci.client.auth.fingerprint", oci_config["fingerprint"])
        conf.set("fs.oci.client.auth.pemfilepath", oci_config["key_file"])
        conf.set(
            "fs.oci.client.hostname",
            "https://objectstorage.{0}.oraclecloud.com".format(
                oci_config["region"]),
        )
        spark_builder=SparkSession.builder.appName(
            app_name).config(conf=conf)

    # Add in extra configuration.
    for key, val in spark_config.items():
        spark_builder.config(key, val)

    # Create the Spark session.
    session=spark_builder.enableHiveSupport().getOrCreate()
    return session


def in_dataflow():
    """
    Determine if we are running in OCI Data Flow by checking the environment.
    """
    if os.environ.get("HOME") == "/home/dataflow":
        return True
    return False   

def queryMetaStore(spark_session, db_name):

     spark_session.sql("USE " + db_name)
     
     df = spark_session.table(db_name + ".yelp_business_raw")
     df.printSchema()
     print("####Show the Name of the Restaurants###\n")
     df.select("name").show()
     print("####Show the Name of the Restaurants having more than 300 review ###\n")
     df.filter(df['review_count'] > 300).show()
     print("####Count of review by City ###\n")
     df.groupBy("city").count().sort(desc("count")).show()

if __name__ == "__main__":
    main()
</copy>    
```

2. Above file is already present in bucket ```workshop-scripts``` as ```query_metastore.py```

![Query MetaStore ](../images/query-metastore.png " ")

3. Create a Python Application in OCI Dataflow as described in the Step #4 above. Provide the following input :

   1. ```Name``` for the application

   2. Select the ```query_metastore.py``` from the bucket ```workshop-scripts```

   3. In the argument Pass the DBName that used to create the Managed Table.

   4. In the `Metastore in dataflow-demo` choose ```DF-metastore``` and you should the path of the Managed table getting populated automatically.

   5. The configuration should look like as shown in the snapshot

       ![Query MetaStore Application ](../images/query-metastore-application-1.png " ")

       ![Query MetaStore Application ](../images/query-metastore-application-2.png " ")

   6. Click ```Create``` and save the application.

   7. Next Run the Application, by launching the application and clicking on ```Run```

      ![Query MetaStore Application Run ](../images/Query-metastore-run-application.png " ")

   8. Application status changes from ```Accepted``` to ```In-Progress``` and ```Succeeded```

   9. Open the logs and should see the results as shown below

    ![Query MetaStore Logs ](../images/Query-metastore-run-application.png " ")

## **STEP 7**: Connect to the Managed Table from Seperate Application


## Acknowledgements

* **Author** - Anand Chandak

* **Adapted by** -  
* **Contributors** -

* **Last Updated By/Date** -

## Need Help?

Please submit feedback or ask for help using our [LiveLabs Support Forum](<https://community.oracle.com/tech/developers/categories/Data> flow). Please click the **Log In** button and login using your Oracle Account. Click the **Ask A Question** button to the left to start a *New Discussion* or *Ask a Question*.  Please include your workshop name and lab name.  You can also include screenshots and attach files.  Engage directly with the author of the workshop.

If you do not have an Oracle Account, [click](https://profile.oracle.com/myprofile/account/create-account.jspx) to create one.
